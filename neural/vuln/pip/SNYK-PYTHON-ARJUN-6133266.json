{
    "CVSSv3": {
        "CVSS": "3.1",
        "attackVector": "NETWORK",
        "attackComplexity": "LOW",
        "privilegesRequired": "NONE",
        "userInteraction": "NONE",
        "scope": "UNCHANGED",
        "confidentiality": "NONE",
        "integrity": "NONE",
        "availability": "LOW"
    },
    "credit": [
        "Unknown"
    ],
    "cvssDetails": [],
    "cvssScore": 5.3,
    "disclosureTime": "2023-12-01 06:27:46",
    "epssDetails": null,
    "exploitMaturity": "Not Defined",
    "id": "SNYK-PYTHON-ARJUN-6133266",
    "identifiers": {
        "CVE": [],
        "CWE": [
            "CWE-1333"
        ],
        "PVE": [
            "PVE-2023-62351"
        ]
    },
    "language": "python",
    "malicious": false,
    "packageManager": "pip",
    "publicationTime": "2023-12-20 15:52:13",
    "remediation": "Upgrade arjun to version 2.1.5 or higher. ",
    "severity": "medium",
    "socialTrendAlert": false,
    "title": "Regular Expression Denial of Service (ReDoS)",
    "vulnDescription": {
        "Details": "Denial of Service (DoS) describes a family of attacks, all aimed at making a system inaccessible to its original and legitimate users. There are many types of DoS attacks, ranging from trying to clog the network pipes to the system by generating a large volume of traffic from many machines (a Distributed Denial of Service - DDoS - attack) to sending crafted requests that cause a system to crash or take a disproportional amount of time to process. The Regular expression Denial of Service (ReDoS) is a type of Denial of Service attack. Regular expressions are incredibly powerful, but they aren't very intuitive and can ultimately end up making it easy for attackers to take your site down. Let\u2019s take the following regular expression as an example: regex = /A(B|C+)+D/ This regular expression accomplishes the following: A The string must start with the letter 'A' (B|C+)+ The string must then follow the letter A with either the letter 'B' or some number of occurrences of the letter 'C' (the + matches one or more times). The + at the end of this section states that we can look for one or more matches of this section. D Finally, we ensure this section of the string ends with a 'D' The expression would match inputs such as ABBD , ABCCCCD , ABCBCCCD and ACCCCCD It most cases, it doesn't take very long for a regex engine to find a match: $ time node -e '/A(B|C+)+D/.test(\"ACCCCCCCCCCCCCCCCCCCCCCCCCCCCD\")' 0.04s user 0.01s system 95% cpu 0.052 total $ time node -e '/A(B|C+)+D/.test(\"ACCCCCCCCCCCCCCCCCCCCCCCCCCCCX\")' 1.79s user 0.02s system 99% cpu 1.812 total The entire process of testing it against a 30 characters long string takes around ~52ms. But when given an invalid string, it takes nearly two seconds to complete the test, over ten times as long as it took to test a valid string. The dramatic difference is due to the way regular expressions get evaluated. Most Regex engines will work very similarly (with minor differences). The engine will match the first possible way to accept the current character and proceed to the next one. If it then fails to match the next one, it will backtrack and see if there was another way to digest the previous character. If it goes too far down the rabbit hole only to find out the string doesn\u2019t match in the end, and if many characters have multiple valid regex paths, the number of backtracking steps can become very large, resulting in what is known as catastrophic backtracking . Let's look at how our expression runs into this problem, using a shorter string: \"ACCCX\". While it seems fairly straightforward, there are still four different ways that the engine could match those three C's: CCC CC+C C+CC C+C+C. The engine has to try each of those combinations to see if any of them potentially match against the expression. When you combine that with the other steps the engine must take, we can use RegEx 101 debugger to see the engine has to take a total of 38 steps before it can determine the string doesn't match. From there, the number of steps the engine must use to validate a string just continues to grow. String Number of C's Number of steps ACCCX 3 38 ACCCCX 4 71 ACCCCCX 5 136 ACCCCCCCCCCCCCCX 14 65,553 By the time the string includes 14 C's, the engine has to take over 65,000 steps just to see if the string is valid. These extreme situations can cause them to work very slowly (exponentially related to input size, as shown above), allowing an attacker to exploit this and can cause the service to excessively consume CPU, resulting in a Denial of Service. ",
        "Overview": "arjun is a HTTP parameter discovery suite Affected versions of this package are vulnerable to Regular Expression Denial of Service (ReDoS) due to the usage of an insecure regular expression in heuristic.py "
    },
    "source_code": [
        {
            "filename": "arjun/__main__.py",
            "diff": "@@ -25,16 +25,17 @@\n parser.add_argument('-oB', help='Port for output to Burp Suite Proxy. Default port is 8080.', dest='burp_port', nargs='?', const=8080)\n parser.add_argument('-d', help='Delay between requests in seconds. (default: 0)', dest='delay', type=float, default=0)\n parser.add_argument('-t', help='Number of concurrent threads. (default: 2)', dest='threads', type=int, default=2)\n-parser.add_argument('-w', help='Wordlist file path. (default: {arjundir}/db/default.txt)', dest='wordlist', default=arjun_dir+'/db/default.txt')\n+parser.add_argument('-w', help='Wordlist file path. (default: {arjundir}/db/large.txt)', dest='wordlist', default=arjun_dir+'/db/large.txt')\n parser.add_argument('-m', help='Request method to use: GET/POST/XML/JSON. (default: GET)', dest='method', default='GET')\n parser.add_argument('-i', help='Import target URLs from file.', dest='import_file', nargs='?', const=True)\n parser.add_argument('-T', help='HTTP request timeout in seconds. (default: 15)', dest='timeout', type=float, default=15)\n-parser.add_argument('-c', help='Chunk size. The number of parameters to be sent at once', type=int, dest='chunks', default=500)\n+parser.add_argument('-c', help='Chunk size. The number of parameters to be sent at once', type=int, dest='chunks', default=300)\n parser.add_argument('-q', help='Quiet mode. No output.', dest='quiet', action='store_true')\n parser.add_argument('--headers', help='Add headers. Separate multiple headers with a new line.', dest='headers', nargs='?', const=True)\n parser.add_argument('--passive', help='Collect parameter names from passive sources like wayback, commoncrawl and otx.', dest='passive', nargs='?', const='-')\n parser.add_argument('--stable', help='Prefer stability over speed.', dest='stable', action='store_true')\n parser.add_argument('--include', help='Include this data in every request.', dest='include', default={})\n+parser.add_argument('--disable-redirects', help='Include this data in every request.', dest='disable_redirects', action='store_true')\n args = parser.parse_args() # arguments to be parsed\n \n if args.quiet:\n@@ -58,6 +59,8 @@\n \n if mem.var['stable'] or mem.var['delay']:\n     mem.var['threads'] = 1\n+if mem.var['wordlist'] in ('large', 'medium', 'small'):\n+    mem.var['wordlist'] = f'{arjun_dir}/db/{mem.var[\"wordlist\"]}.txt'\n \n try:\n     wordlist_file = arjun_dir + '/db/small.txt' if args.wordlist == 'small' else args.wordlist\n@@ -93,8 +96,9 @@ def narrower(request, factors, param_groups):\n     for i, result in enumerate(as_completed(futures)):\n         if result.result():\n             anomalous_params.extend(slicer(result.result()))\n-        if not mem.var['kill']:\n-            print('%s Processing chunks: %i/%-6i' % (info, i + 1, len(param_groups)), end='\\r')\n+        if mem.var['kill']:\n+            return anomalous_params\n+        print('%s Processing chunks: %i/%-6i' % (info, i + 1, len(param_groups)), end='\\r')\n     return anomalous_params\n \n \n@@ -108,8 +112,8 @@ def initialize(request, wordlist):\n         print('%s %s is not a valid URL' % (bad, url))\n         return 'skipped'\n     print('%s Probing the target for stability' % run)\n-    stable = stable_request(url, request['headers'])\n-    if not stable:\n+    request['url'] = stable_request(url, request['headers'])\n+    if not request['url']:\n         return 'skipped'\n     else:\n         fuzz = random_str(6)"
        },
        {
            "filename": "arjun/core/anomaly.py",
            "diff": "@@ -1,6 +1,7 @@\n import re\n import requests\n \n+from urllib.parse import urlparse\n from arjun.core.utils import diff_map, remove_tags\n \n \n@@ -26,8 +27,9 @@ def define(response_1, response_2, param, value, wordlist):\n             factors['same_code'] = response_1.status_code\n         if response_1.headers.keys() == response_2.headers.keys():\n             factors['same_headers'] = list(response_1.headers.keys())\n-        if response_1.url == response_2.url:\n-            factors['same_redirect'] = response_1.url\n+            factors['same_headers'].sort()\n+        if response_1.headers.get('Location', '') == response_2.headers.get('Location', ''):\n+            factors['same_redirect'] = urlparse(response_1.headers.get('Location', '')).path\n         if response_1.text == response_2.text:\n             factors['same_body'] = response_1.text\n         elif response_1.text.count('\\n') == response_2.text.count('\\n'):\n@@ -48,11 +50,13 @@ def compare(response, factors, params):\n     detects anomalies by comparing a HTTP response against a rule list\n     returns string, list (anomaly, list of parameters that caused it)\n     \"\"\"\n+    these_headers = list(response.headers.keys())\n+    these_headers.sort()\n     if factors['same_code'] and response.status_code != factors['same_code']:\n         return ('http code', params)\n-    if factors['same_headers'] and list(response.headers.keys()) != factors['same_headers']:\n+    if factors['same_headers'] and these_headers != factors['same_headers']:\n         return ('http headers', params)\n-    if factors['same_redirect'] and response.url != factors['same_redirect']:\n+    if factors['same_redirect'] and urlparse(response.headers.get('Location', '')).path != factors['same_redirect']:\n         return ('redirection', params)\n     if factors['same_body'] and response.text != factors['same_body']:\n         return ('body length', params)\n@@ -66,7 +70,9 @@ def compare(response, factors, params):\n                 return ('lines', params)\n     if type(factors['param_missing']) == list:\n         for param in params.keys():\n-            if param in response.text and param not in factors['param_missing'] and re.search(r'[\\'\"\\s]%s[\\'\"\\s]' % param, response.text):\n+            if len(param) < 5:\n+                continue\n+            if param not in factors['param_missing'] and re.search(r'[\\'\"\\s]%s[\\'\"\\s]' % param, response.text):\n                 return ('param name reflection', params)\n     if factors['value_missing']:\n         for value in params.values():"
        },
        {
            "filename": "arjun/core/bruter.py",
            "diff": "@@ -17,6 +17,7 @@ def bruter(request, factors, params, mode='bruteforce'):\n     if conclusion == 'retry':\n         return bruter(request, factors, params, mode=mode)\n     elif conclusion == 'kill':\n+        mem.var['kill'] = True\n         return []\n     comparison_result = compare(response, factors, params)\n     if mode == 'verify':"
        },
        {
            "filename": "arjun/core/error_handler.py",
            "diff": "@@ -25,20 +25,21 @@ def error_handler(response, factors):\n \t\t'kill': stop processing this target\n \treturns str\n \t\"\"\"\n-\tif type(response) != str and response.status_code in (400, 503, 429):\n-\t\tif response.status_code == 400:\n-\t\t\tif factors['same_code'] != 400:\n+\tif type(response) != str and response.status_code in (400, 413, 418, 429, 503):\n+\t\tif response.status_code == 503:\n+\t\t\tmem.var['kill'] = True\n+\t\t\tprint('%s Target is unable to process requests, try --stable switch' % bad)\n+\t\t\treturn 'kill'\n+\t\telif response.status_code in (429, 418):\n+\t\t\tprint('%s Target has a rate limit in place, try --stable switch' % bad)\n+\t\t\treturn 'kill'\n+\t\telse:\n+\t\t\tif factors['same_code'] != response.status_code:\n \t\t\t\tmem.var['kill'] = True\n \t\t\t\tprint('%s Server received a bad request. Try decreasing the chunk size with -c option' % bad)\n \t\t\t\treturn 'kill'\n \t\t\telse:\n \t\t\t\treturn 'ok'\n-\t\telif response.status_code == 503:\n-\t\t\tmem.var['kill'] = True\n-\t\t\tprint('%s Target is unable to process requests, try --stable switch' % bad)\n-\t\t\treturn 'kill'\n-\t\telif response.status_code == 429:\n-\t\t\treturn connection_refused()\n \telse:\n \t\tif 'Timeout' in response:\n \t\t\tif mem.var['timeout'] > 20:"
        },
        {
            "filename": "arjun/core/requester.py",
            "diff": "@@ -25,20 +25,52 @@ def requester(request, payload={}):\n         return 'killed'\n     try:\n         if request['method'] == 'GET':\n-            response = requests.get(url, params=payload, headers=request['headers'], verify=False, timeout=mem.var['timeout'])\n+            response = requests.get(url,\n+                params=payload,\n+                headers=request['headers'],\n+                verify=False,\n+                allow_redirects=False,\n+                timeout=mem.var['timeout'],\n+            )\n         elif request['method'] == 'JSON':\n             request['headers']['Content-Type'] = 'application/json'\n             if mem.var['include'] and '$arjun$' in mem.var['include']:\n-                payload = mem.var['include'].replace('$arjun$', json.dumps(payload).rstrip('}').lstrip('{'))\n-                response = requests.post(url, data=payload, headers=request['headers'], verify=False, timeout=mem.var['timeout'])\n+                payload = mem.var['include'].replace('$arjun$',\n+                    json.dumps(payload).rstrip('}').lstrip('{'))\n+                response = requests.post(url,\n+                    data=payload,\n+                    headers=request['headers'],\n+                    verify=False,\n+                    allow_redirects=False,\n+                    timeout=mem.var['timeout'],\n+                )\n             else:\n-                response = requests.post(url, json=payload, headers=request['headers'], verify=False, timeout=mem.var['timeout'])\n+                response = requests.post(url,\n+                    json=payload,\n+                    headers=request['headers'],\n+                    verify=False,\n+                    allow_redirects=False,\n+                    timeout=mem.var['timeout'],\n+                )\n         elif request['method'] == 'XML':\n             request['headers']['Content-Type'] = 'application/xml'\n-            payload = mem.var['include'].replace('$arjun$', dict_to_xml(payload))\n-            response = requests.post(url, data=payload, headers=request['headers'], verify=False, timeout=mem.var['timeout'])\n+            payload = mem.var['include'].replace('$arjun$',\n+                dict_to_xml(payload))\n+            response = requests.post(url,\n+                data=payload,\n+                headers=request['headers'],\n+                verify=False,\n+                allow_redirects=False,\n+                timeout=mem.var['timeout'],\n+            )\n         else:\n-            response = requests.post(url, data=payload, headers=request['headers'], verify=False, timeout=mem.var['timeout'])\n+            response = requests.post(url,\n+                data=payload,\n+                headers=request['headers'],\n+                verify=False,\n+                allow_redirects=False,\n+                timeout=mem.var['timeout'],\n+            )\n         return response\n     except Exception as e:\n         return str(e)"
        },
        {
            "filename": "arjun/core/utils.py",
            "diff": "@@ -66,15 +66,18 @@ def stable_request(url, headers):\n     returns None in case of failure, returns a \"response\" object otherwise\n     \"\"\"\n     parsed = urlparse(url)\n+    redirects_allowed = False if mem.var['disable_redirects'] else True\n     scheme, host, path = parsed.scheme, parsed.netloc, parsed.path\n     schemes = (['https', 'http'] if scheme == 'https' else ['http', 'https'])\n     for scheme in schemes:\n         try:\n-            return requests.get(\n+            response = requests.get(\n                 scheme + '://' + host + path,\n                 headers=headers,\n                 verify=False,\n-                timeout=10).status_code\n+                timeout=10,\n+                allow_redirects=redirects_allowed)\n+            return response.url\n         except Exception as e:\n             if 'ConnectionError' not in str(e):\n                 continue\n@@ -160,13 +163,16 @@ def reader(path, mode='string'):\n             return ''.join([line for line in file])\n \n \n-re_extract_js = re.compile(r'(?si)<script[^>]*>([^<].+?)</script')\n def extract_js(response):\n     \"\"\"\n     extracts javascript from a given string\n     \"\"\"\n-    return re_extract_js.findall(response)\n-\n+    scripts = []\n+    for part in re.split('(?i)<script[> ]', response):\n+        actual_parts = re.split('(?i)</script>', part, maxsplit=2)\n+        if len(actual_parts) > 1:\n+            scripts.append(actual_parts[0])\n+    return scripts\n \n def parse_headers(string):\n     \"\"\""
        },
        {
            "filename": "arjun/plugins/heuristic.py",
            "diff": "@@ -9,8 +9,8 @@ def is_not_junk(param):\n # TODO: for map keys, javascript tolerates { param: \"value\" }\n re_input_names = re.compile(r'''(?i)<input.+?name=[\"']?([^\"'\\s>]+)''')\n re_input_ids = re.compile(r'''(?i)<input.+?id=[\"']?([^\"'\\s>]+)''')\n-re_empty_vars = re.compile(r'''([^\\s!=<>]+)\\s*=\\s*(?:['\"`]{2}|true|false|null)''')\n-re_map_keys = re.compile(r'''([^'\"]+)['\"]\\s*:\\s*['\"`]''')\n+re_empty_vars = re.compile(r'''(?:[;\\n]|\\bvar|\\blet)(\\w+)\\s*=\\s*(?:['\"`]{1,2}|true|false|null)''')\n+re_map_keys = re.compile(r'''['\"](\\w+?)['\"]\\s*:\\s*['\"`]''')\n def heuristic(response, wordlist):\n     potential_params = []\n "
        }
    ],
    "commitTime": "2022-04-04 09:17:27"
}