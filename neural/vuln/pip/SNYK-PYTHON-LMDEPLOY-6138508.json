{
    "CVSSv3": {
        "CVSS": "3.1",
        "attackVector": "NETWORK",
        "attackComplexity": "HIGH",
        "privilegesRequired": "NONE",
        "userInteraction": "NONE",
        "scope": "UNCHANGED",
        "confidentiality": "LOW",
        "integrity": "NONE",
        "availability": "HIGH"
    },
    "credit": [
        "akhoroshev"
    ],
    "cvssDetails": [],
    "cvssScore": 6.5,
    "disclosureTime": "2023-10-01 06:34:29",
    "epssDetails": null,
    "exploitMaturity": "Not Defined",
    "id": "SNYK-PYTHON-LMDEPLOY-6138508",
    "identifiers": {
        "CVE": [],
        "CWE": [
            "CWE-362"
        ],
        "PVE": [
            "PVE-2023-61437"
        ]
    },
    "language": "python",
    "malicious": false,
    "packageManager": "pip",
    "publicationTime": "2023-12-22 14:16:38",
    "remediation": "Upgrade lmdeploy to version 0.0.10 or higher. ",
    "severity": "medium",
    "socialTrendAlert": false,
    "title": "Race Condition",
    "vulnDescription": {
        "Overview": "lmdeploy is a Affected versions of this package are vulnerable to Race Condition. when generating multiple requests with tensor para > 1 . "
    },
    "source_code": [
        {
            "filename": "src/turbomind/models/llama/LlamaBatch.cc",
            "diff": "@@ -30,6 +30,9 @@ void LlamaBatch<T>::verifyRequests(std::vector<std::shared_ptr<Request>>& stop_r\n \n     auto invalidate = [](const char* type, std::shared_ptr<Request>& req, int ec) {\n         TM_LOG_WARNING(\"[verifyRequests] Skipping invalid %s request for id %ld, code = %d\", type, (long)req->id, ec);\n+        // We don't need a barrier there because\n+        // this lambda is called only for new requests\n+        // which are visible only for rank = 0 thread.\n         req->signal.set_value(ec);\n         req.reset();\n     };\n@@ -139,6 +142,12 @@ void LlamaBatch<T>::handleStopRequests(const std::vector<std::shared_ptr<Request\n             check_cuda_error(cudaMemsetAsync(sequence_length.getPtr<int>(), 0, sizeof(int), stream_));\n             check_cuda_error(cudaStreamSynchronize(stream_));\n         }\n+\n+        // When the signal is set threads from LlamaV2::forward can exit\n+        // and free inputs/outputs tensors.\n+        // Therefore we need to make sure that no threads from LlamaV2::internalThreadEntry\n+        // are accessing the tensors.\n+        llama_->shared_state_->barrier->wait();\n         if (rank_ == 0) {\n             r->signal.set_value(ec);\n         }\n@@ -1112,6 +1121,11 @@ void LlamaBatch<T>::finishRequest(int index, bool force_end)\n         llama_->kv_cache_mgr_->update(cached_seq_[index], stream_);\n     }\n \n+    // When the signal is set threads from LlamaV2::forward can exit\n+    // and free inputs/outputs tensors.\n+    // Therefore we need to make sure that no threads from LlamaV2::internalThreadEntry\n+    // are accessing the tensors.\n+    llama_->shared_state_->barrier->wait();\n     if (rank_ == 0) {\n         requests_[index]->signal.set_value(0);\n     }"
        }
    ],
    "commitTime": "2023-09-26 04:45:02"
}